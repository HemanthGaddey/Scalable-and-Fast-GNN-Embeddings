{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bolt17/miniconda3/envs/StableVITON/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Flickr, Coauthor, CoraFull, Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(G, color):\n",
    "    plt.figure(figsize=(103,103))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_embedding(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    h = h.detach().cpu().numpy()\n",
    "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "    if epoch is not None and loss is not None:\n",
    "        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x7FB521CA4D40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------SEED------------------\n",
    "SEED=548\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import sklearn\n",
    "sklearn.utils.check_random_state(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "def edge_index_to_adj(edge_index, num_nodes):\n",
    "    values = torch.ones(edge_index.shape[1])\n",
    "    adj_matrix = torch.sparse_coo_tensor(edge_index, values, (num_nodes, num_nodes))\n",
    "    return adj_matrix.to_dense()\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
    "data = dataset[0]  # The dataset contains a single graph\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum cross similarity:25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_302544/244665361.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features = torch.tensor(data.x)\n"
     ]
    }
   ],
   "source": [
    "adj_matrix = edge_index_to_adj(data.edge_index, len(data.x))\n",
    "\n",
    "features = torch.tensor(data.x)\n",
    "# finding feature similarity across all the nodes via dot product\n",
    "similarities = (features@features.T)\n",
    "\n",
    "# making the diagnol elements to zero as the max similarity is obtained with a same\n",
    "similarities = similarities * (torch.eye(len(similarities)) == 0).long()\n",
    "\n",
    "maxi=similarities.max()\n",
    "print(f\"maximum cross similarity:{maxi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:0.25\n"
     ]
    }
   ],
   "source": [
    "# normalizing similarty to lie in range [0,1]\n",
    "similarities=torch.div(similarities, maxi)\n",
    "\n",
    "alpha = 0.25\n",
    "print(f\"alpha:{alpha}\")\n",
    "max_similarities = (similarities > alpha).long()\n",
    "\n",
    "attr_edges = torch.nonzero(max_similarities, as_tuple=False).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------mapping attr edges to virtual indices-----------------\n",
    "connected_nodes = torch.unique(attr_edges)\n",
    "map_attr_edges={idx.item():i for i, idx in enumerate(connected_nodes)}\n",
    "rev_map_attr_edges={j:i for i,j in map_attr_edges.items()}\n",
    "mapped_attr_edges = torch.tensor([map_attr_edges[idx.item()] for idx in attr_edges.flatten()], device=attr_edges.device).reshape(attr_edges.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2698, 2700, 2701, 2703, 2705], 1434)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map_attr_edges.keys())[-5:], len(map_attr_edges.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected nodes (1434))\n",
      "Disconnected nodes (1274)\n",
      "Total edges in structure network created: 10556\n",
      "Total edges in attribute network created: 4914\n"
     ]
    }
   ],
   "source": [
    "# Find all nodes in the graph\n",
    "all_nodes = torch.arange(len(data.x))\n",
    "\n",
    "# Find disconnected nodes (nodes not present in attr_edges)\n",
    "disconnected_nodes = torch.tensor([node for node in all_nodes if node not in connected_nodes])\n",
    "\n",
    "print(f\"Connected nodes ({len(connected_nodes)}))\")\n",
    "print(f\"Disconnected nodes ({len(disconnected_nodes)})\")\n",
    "\n",
    "print(f\"Total edges in structure network created: {data.edge_index.shape[1]}\")\n",
    "print(f\"Total edges in attribute network created: {attr_edges.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_edges = lambda args: pd.DataFrame(args[0].T).to_csv(args[1],index=None, header=None, sep=' ', mode='a')\n",
    "def get_embeddings(\n",
    "    edges, \n",
    "    edges_name=\"edgelist.txt\", \n",
    "    k=128,  # Embedding dimension\n",
    "    a=0.01,  # damping parameter\n",
    "    partition=1,\n",
    "    output=False,\n",
    "    hi2vec_file=\"hi2vec\"\n",
    "):\n",
    "    '''\n",
    "    \"partition\": the partition algorithm to use, default is 1.\n",
    "    0: random bisection\n",
    "    1: Louvain partition\n",
    "    2: Louvain first-level partition\n",
    "    3: Label Propagation partition\n",
    "    '''\n",
    "    open(edges_name,'w').close()\n",
    "    save_edges((edges, edges_name))\n",
    "\n",
    "    if not os.path.exists('./hierarchy.txt'):\n",
    "        os.mknod('./hierarchy.txt')\n",
    "    else:\n",
    "        open('hierarchy.txt','w').close()\n",
    "    if not os.path.exists('./vectors.txt'):\n",
    "        os.mknod('./vectors.txt')\n",
    "    else:\n",
    "        open('vectors.txt','w').close()\n",
    "    \n",
    "    # do hierarchical clustering using Louvain algorithm\n",
    "    endstr = '> /dev/null 2>&1'\n",
    "    if(output):\n",
    "        endstr = ''\n",
    "    os.system(f'./LouvainNE/recpart ./{edges_name} ./hierarchy.txt {partition} {endstr}')\n",
    "    \n",
    "    # obtain node embedding of each node at every hierarchy\n",
    "    os.system(f'./LouvainNE/{hi2vec_file} {k} {a} ./hierarchy.txt ./vectors.txt {endstr}')\n",
    "    \n",
    "    # Path to your output node embeddings text file\n",
    "    file_path = 'vectors.txt'\n",
    "\n",
    "    data_ = np.loadtxt(file_path)\n",
    "    data_tensor = torch.from_numpy(data_)\n",
    "    # The first column contains node IDs\n",
    "    node_ids = data_tensor[:, 0].to(torch.int)\n",
    "    # The remaining columns contain embeddings\n",
    "    embeddings = data_tensor[:, 1:]\n",
    "    \n",
    "    return node_ids, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_node_classification(aa_node_ids, aa_embeddings, data):\n",
    "    \"\"\"\n",
    "    Evaluates multi-class node classification using Logistic Regression.\n",
    "\n",
    "    Args:\n",
    "        aa_node_ids: List of node IDs.\n",
    "        aa_embeddings: Corresponding node embeddings.\n",
    "        data: PyTorch Geometric data object containing labels and masks.\n",
    "\n",
    "    Returns:\n",
    "        Micro-F1 and Macro-F1 scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Map node IDs to indices in embeddings\n",
    "    node_to_idx = {node_id.item(): idx for idx, node_id in enumerate(aa_node_ids)}\n",
    "    # Reorder embeddings to match the label ordering\n",
    "    ordered_embeddings = np.zeros((len(data.y), aa_embeddings.shape[1]))\n",
    "    for i in range(len(data.y)):\n",
    "        ordered_embeddings[i] = aa_embeddings[node_to_idx[i]]\n",
    "\n",
    "    # Convert masks to NumPy arrays\n",
    "    train_mask = data.train_mask.numpy()\n",
    "    test_mask = data.test_mask.numpy()\n",
    "\n",
    "    # Prepare train and test data\n",
    "    X_train, X_test = ordered_embeddings[train_mask], ordered_embeddings[test_mask]\n",
    "    y_train, y_test = data.y[train_mask].numpy(), data.y[test_mask].numpy()\n",
    "\n",
    "    # Train Logistic Regression classifier\n",
    "    clf = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Compute Micro-F1 and Macro-F1 scores\n",
    "    micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    return {\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1\n",
    "    }\n",
    "\n",
    "def evaluate_node_classification_latefusion(aa_node_ids_a, aa_embeddings_a, aa_node_ids_b, aa_embeddings_b, data, agg_type):\n",
    "    \"\"\"\n",
    "    Evaluates multi-class node classification using Logistic Regression.\n",
    "\n",
    "    Args:\n",
    "        aa_node_ids: List of node IDs.\n",
    "        aa_embeddings: Corresponding node embeddings.\n",
    "        data: PyTorch Geometric data object containing labels and masks.\n",
    "\n",
    "    Returns:\n",
    "        Micro-F1 and Macro-F1 scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Map node IDs to indices in embeddings\n",
    "    node_to_idx_a = {node_id.item(): idx for idx, node_id in enumerate(aa_node_ids_a)}\n",
    "    node_to_idx_b = {node_id.item(): idx for idx, node_id in enumerate(aa_node_ids_b)}\n",
    "\n",
    "    # Reorder embeddings to match the label ordering\n",
    "    ordered_embeddings_a = np.zeros((len(data.y), aa_embeddings_a.shape[1]))\n",
    "    ordered_embeddings_b = np.zeros((len(data.y), aa_embeddings_b.shape[1]))\n",
    "\n",
    "    for i in range(len(data.y)):\n",
    "        ordered_embeddings_a[i] = aa_embeddings_a[node_to_idx_a[i]]\n",
    "        ordered_embeddings_b[i] = aa_embeddings_b[node_to_idx_b[i]]\n",
    "\n",
    "    ordered_embeddings = np.zeros((len(data.y), aa_embeddings_a.shape[1]))\n",
    "    if agg_type == 'sum':\n",
    "        ordered_embeddings = ordered_embeddings_a+ordered_embeddings_b\n",
    "    elif agg_type == 'concat':\n",
    "        if isinstance(ordered_embeddings_a, np.ndarray):\n",
    "            ordered_embeddings_a = torch.tensor(ordered_embeddings_a, dtype=torch.float32)\n",
    "\n",
    "        if isinstance(ordered_embeddings_b, np.ndarray):\n",
    "            ordered_embeddings_b = torch.tensor(ordered_embeddings_b, dtype=torch.float32)\n",
    "        ordered_embeddings = torch.cat([ordered_embeddings_a, ordered_embeddings_b],dim=1)\n",
    "    elif agg_type == 'avg':\n",
    "        ordered_embeddings = (ordered_embeddings_a+ordered_embeddings_b)/2\n",
    "    else: \n",
    "        raise ValueError('Invalid agg_type, choose from sum/concat/avg')\n",
    "    # Convert masks to NumPy arrays\n",
    "    train_mask = data.train_mask.numpy()\n",
    "    test_mask = data.test_mask.numpy()\n",
    "\n",
    "    # Prepare train and test data\n",
    "    X_train, X_test = ordered_embeddings[train_mask], ordered_embeddings[test_mask]\n",
    "    y_train, y_test = data.y[train_mask].numpy(), data.y[test_mask].numpy()\n",
    "\n",
    "    # Train Logistic Regression classifier\n",
    "    clf = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Compute Micro-F1 and Macro-F1 scores\n",
    "    micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    return {\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append(idxs, emb, data):\n",
    "    l=data.x.shape[0]\n",
    "    ids=[i for i in range(data.x.shape[0])]\n",
    "    final_ids=copy.deepcopy(idxs)\n",
    "    final_ids=final_ids.tolist()\n",
    "    final_emb = copy.deepcopy(emb)\n",
    "    final_emb=final_emb.tolist()\n",
    "    for i in ids:\n",
    "        if i in final_ids:\n",
    "            continue\n",
    "        else:\n",
    "            final_ids.append(i)\n",
    "            final_emb.append(torch.zeros(len(final_emb[0])))\n",
    "    final_ids=torch.tensor(final_ids)\n",
    "    final_emb=torch.tensor(final_emb)\n",
    "    return final_ids, final_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On  Structural Graph:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-F1: 0.5312 ± 0.0070\n",
      "Macro-F1: 0.5357 ± 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('On  Structural Graph:')\n",
    "struc_edges = data.edge_index\n",
    "a = []\n",
    "b = []\n",
    "for i in tqdm(range(12)):\n",
    "    aa_node_ids, aa_embeddings = get_embeddings(struc_edges, \"struc_edgelist_late_fusion.txt\", k=256, partition=1,hi2vec_file='hi2vec_mod')\n",
    "    result = evaluate_node_classification(aa_node_ids, aa_embeddings, data)\n",
    "    a.append(result['micro_f1'])\n",
    "    b.append(result['macro_f1'])\n",
    "\n",
    "print(f\"Micro-F1: {np.mean(a):.4f} ± {np.std(a):.4f}\")\n",
    "print(f\"Macro-F1: {np.mean(b):.4f} ± {np.std(b):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Late Fusion (Sum):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:30<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-F1: 0.5134 ± 0.0053\n",
      "Macro-F1: 0.5048 ± 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Late Fusion (Sum):')\n",
    "a = []\n",
    "b = []\n",
    "for i in tqdm(range(32)):\n",
    "    aa_node_ids_struc, aa_embeddings_struc = get_embeddings(struc_edges, \"struc_edgelist_late_fusion.txt\", k=512, partition=1,hi2vec_file='hi2vec')\n",
    "    aa_node_ids_attr, aa_embeddings_attr = get_embeddings(mapped_attr_edges, \"attr_edgelist_late_fusion.txt\", k=512, partition=1,hi2vec_file='hi2vec') # hi2vec_mod\n",
    "    # reverse mapping the indices from virtual index to actual index\n",
    "    org_aa_node_ids_attr = torch.tensor([rev_map_attr_edges[idx.item()] for idx in aa_node_ids_attr])\n",
    "    aa_node_ids_attr, aa_embeddings_attr = append(org_aa_node_ids_attr, aa_embeddings_attr, data)\n",
    "    agg_type='concat' # sum/concat/avg\n",
    "    result = evaluate_node_classification_latefusion(aa_node_ids_struc, aa_embeddings_struc, aa_node_ids_attr, aa_embeddings_attr, data, agg_type)\n",
    "    a.append(result['micro_f1'])\n",
    "    b.append(result['macro_f1'])\n",
    "\n",
    "print(f\"Micro-F1: {np.mean(a):.4f} ± {np.std(a):.4f}\")\n",
    "print(f\"Macro-F1: {np.mean(b):.4f} ± {np.std(b):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_node_ids_struc, aa_embeddings_struc = get_embeddings(struc_edges, \"struc_edgelist_late_fusion.txt\", k=256, partition=1)\n",
    "aa_node_ids_attr, aa_embeddings_attr = get_embeddings(mapped_attr_edges, \"attr_edgelist_late_fusion.txt\", k=256, partition=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708]) torch.Size([2708, 256])\n",
      "torch.Size([1434]) torch.Size([1434, 256])\n"
     ]
    }
   ],
   "source": [
    "print(aa_node_ids_struc.shape, aa_embeddings_struc.shape)\n",
    "print(aa_node_ids_attr.shape, aa_embeddings_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse mapping the indices from virtual index to actual index\n",
    "org_aa_node_ids_attr = torch.tensor([rev_map_attr_edges[idx.item()] for idx in aa_node_ids_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,  189,   29, 1296,  115,  769,  870, 1181, 1276,    9],\n",
      "       dtype=torch.int32)\n",
      "tensor([   1,  332,   48, 2406,  197, 1446, 1615, 2214, 2381,   17])\n"
     ]
    }
   ],
   "source": [
    "print(aa_node_ids_attr[:10])\n",
    "print(org_aa_node_ids_attr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_node_ids_attr, aa_embeddings_attr = append(org_aa_node_ids_attr, aa_embeddings_attr, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_type='concat' # sum/concat/avg\n",
    "result = evaluate_node_classification_latefusion(aa_node_ids_struc, aa_embeddings_struc, aa_node_ids_attr, aa_embeddings_attr, data, agg_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'micro_f1': 0.636, 'macro_f1': 0.6343200813275683}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StableVITON",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
