{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bolt17/miniconda3/envs/StableVITON/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Flickr, Coauthor, CoraFull, Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(G, color):\n",
    "    plt.figure(figsize=(103,103))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_embedding(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    h = h.detach().cpu().numpy()\n",
    "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "    if epoch is not None and loss is not None:\n",
    "        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x7F4F5F8F0B40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------SEED------------------\n",
    "SEED=548\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import sklearn\n",
    "sklearn.utils.check_random_state(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "def edge_index_to_adj(edge_index, num_nodes):\n",
    "    values = torch.ones(edge_index.shape[1])\n",
    "    adj_matrix = torch.sparse_coo_tensor(edge_index, values, (num_nodes, num_nodes))\n",
    "    return adj_matrix.to_dense()\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
    "data = dataset[0]  # The dataset contains a single graph\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum cross similarity:25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19839/244665361.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features = torch.tensor(data.x)\n"
     ]
    }
   ],
   "source": [
    "adj_matrix = edge_index_to_adj(data.edge_index, len(data.x))\n",
    "\n",
    "features = torch.tensor(data.x)\n",
    "# finding feature similarity across all the nodes via dot product\n",
    "similarities = (features@features.T)\n",
    "\n",
    "# making the diagnol elements to zero as the max similarity is obtained with a same\n",
    "similarities = similarities * (torch.eye(len(similarities)) == 0).long()\n",
    "\n",
    "maxi=similarities.max()\n",
    "print(f\"maximum cross similarity:{maxi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:[0.5, 0.5]\n",
      "[0.5, 0.5] 2540\n",
      "[0.5, 0.25] 1274\n",
      "[0.25, 0.125] 186\n",
      "[0.125, 0.0625] 3\n",
      "[0.0625, 0.03125] 0\n",
      "[0.03125, 0.046875] 3\n",
      "suitable alpha: 0.03125\n"
     ]
    }
   ],
   "source": [
    "# normalizing similarty to lie in range [0,1]\n",
    "similarities=torch.div(similarities, maxi)\n",
    "\n",
    "alpha = [0.5, 0.5]\n",
    "print(f\"alpha:{alpha}\")\n",
    "f=0\n",
    "patience=0\n",
    "while f!=1:\n",
    "    max_similarities = (similarities > alpha[-1]).long()\n",
    "\n",
    "    attr_edges = torch.nonzero(max_similarities, as_tuple=False).T\n",
    "\n",
    "    connected_nodes = torch.unique(attr_edges)\n",
    "\n",
    "    # Find all nodes in the graph\n",
    "    all_nodes = torch.arange(len(data.x))\n",
    "\n",
    "    # Find disconnected nodes (nodes not present in attr_edges)\n",
    "    disconnected_nodes = torch.tensor([node for node in all_nodes if node not in connected_nodes])\n",
    "\n",
    "    if len(disconnected_nodes)==0:\n",
    "        print(alpha, len(disconnected_nodes))\n",
    "        patience+=1\n",
    "        alfa=alpha[-1]+0.5*alpha[-1]\n",
    "        alpha.pop(0)\n",
    "        alpha.append(alfa)\n",
    "    else:\n",
    "        if patience>0:\n",
    "            print(alpha, len(disconnected_nodes))\n",
    "            fa=alpha[0]\n",
    "            break\n",
    "        else:\n",
    "            print(alpha, len(disconnected_nodes))\n",
    "            alfa=alpha[-1]-0.5*alpha[-1]\n",
    "            alpha.pop(0)\n",
    "            alpha.append(alfa)\n",
    "print('suitable alpha:', fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum cross similarity:25.0\n",
      "alpha:0.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19839/2011775347.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features = torch.tensor(data.x)\n"
     ]
    }
   ],
   "source": [
    "adj_matrix = edge_index_to_adj(data.edge_index, len(data.x))\n",
    "\n",
    "features = torch.tensor(data.x)\n",
    "# finding feature similarity across all the nodes via dot product\n",
    "similarities = (features@features.T)\n",
    "\n",
    "# making the diagnol elements to zero as the max similarity is obtained with a same\n",
    "similarities = similarities * (torch.eye(len(similarities)) == 0).long()\n",
    "\n",
    "maxi=similarities.max()\n",
    "print(f\"maximum cross similarity:{maxi}\")\n",
    "\n",
    "similarities=torch.div(similarities, maxi)\n",
    "\n",
    "alpha = fa\n",
    "print(f\"alpha:{alpha}\")\n",
    "max_similarities = (similarities > alpha).long()\n",
    "\n",
    "attr_edges = torch.nonzero(max_similarities, as_tuple=False).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connected_nodes = torch.unique(attr_edges)\n",
    "len(connected_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_edges = lambda args: pd.DataFrame(args[0].T).to_csv(args[1],index=None, header=None, sep=' ', mode='a')\n",
    "def get_embeddings(\n",
    "    edges, \n",
    "    edges_name=\"edgelist.txt\", \n",
    "    k=128,  # Embedding dimension\n",
    "    a=0.01,  # damping parameter\n",
    "    partition=1,\n",
    "    output=False\n",
    "):\n",
    "    '''\n",
    "    \"partition\": the partition algorithm to use, default is 1.\n",
    "    0: random bisection\n",
    "    1: Louvain partition\n",
    "    2: Louvain first-level partition\n",
    "    3: Label Propagation partition\n",
    "    '''\n",
    "    open(edges_name,'w').close()\n",
    "    save_edges((edges, edges_name))\n",
    "\n",
    "    if not os.path.exists('./hierarchy.txt'):\n",
    "        os.mknod('./hierarchy.txt')\n",
    "    else:\n",
    "        open('hierarchy.txt','w').close()\n",
    "    if not os.path.exists('./vectors.txt'):\n",
    "        os.mknod('./vectors.txt')\n",
    "    else:\n",
    "        open('vectors.txt','w').close()\n",
    "    \n",
    "    # do hierarchical clustering using Louvain algorithm\n",
    "    endstr = '> /dev/null 2>&1'\n",
    "    if(output):\n",
    "        endstr = ''\n",
    "    os.system(f'./LouvainNE/recpart ./{edges_name} ./hierarchy.txt {partition} {endstr}')\n",
    "    \n",
    "    # obtain node embedding of each node at every hierarchy\n",
    "    os.system(f'./LouvainNE/hi2vec {k} {a} ./hierarchy.txt ./vectors.txt {endstr}')\n",
    "    \n",
    "    # Path to your output node embeddings text file\n",
    "    file_path = 'vectors.txt'\n",
    "\n",
    "    data_ = np.loadtxt(file_path)\n",
    "    data_tensor = torch.from_numpy(data_)\n",
    "    # The first column contains node IDs\n",
    "    node_ids = data_tensor[:, 0].to(torch.int)\n",
    "    # The remaining columns contain embeddings\n",
    "    embeddings = data_tensor[:, 1:]\n",
    "    \n",
    "    return node_ids, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_node_classification(aa_node_ids, aa_embeddings, data):\n",
    "    \"\"\"\n",
    "    Evaluates multi-class node classification using Logistic Regression.\n",
    "\n",
    "    Args:\n",
    "        aa_node_ids: List of node IDs.\n",
    "        aa_embeddings: Corresponding node embeddings.\n",
    "        data: PyTorch Geometric data object containing labels and masks.\n",
    "\n",
    "    Returns:\n",
    "        Micro-F1 and Macro-F1 scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Map node IDs to indices in embeddings\n",
    "    node_to_idx = {node_id.item(): idx for idx, node_id in enumerate(aa_node_ids)}\n",
    "    # Reorder embeddings to match the label ordering\n",
    "    ordered_embeddings = np.zeros((len(data.y), aa_embeddings.shape[1]))\n",
    "    for i in range(len(data.y)):\n",
    "        ordered_embeddings[i] = aa_embeddings[node_to_idx[i]]\n",
    "\n",
    "    # Convert masks to NumPy arrays\n",
    "    train_mask = data.train_mask.numpy()\n",
    "    test_mask = data.test_mask.numpy()\n",
    "\n",
    "    # Prepare train and test data\n",
    "    X_train, X_test = ordered_embeddings[train_mask], ordered_embeddings[test_mask]\n",
    "    y_train, y_test = data.y[train_mask].numpy(), data.y[test_mask].numpy()\n",
    "\n",
    "    # Train Logistic Regression classifier\n",
    "    clf = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Compute Micro-F1 and Macro-F1 scores\n",
    "    micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    return {\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1\n",
    "    }\n",
    "\n",
    "def evaluate_node_classification_latefusion(aa_node_ids_a, aa_embeddings_a, aa_node_ids_b, aa_embeddings_b, data, agg_type):\n",
    "    \"\"\"\n",
    "    Evaluates multi-class node classification using Logistic Regression.\n",
    "\n",
    "    Args:\n",
    "        aa_node_ids: List of node IDs.\n",
    "        aa_embeddings: Corresponding node embeddings.\n",
    "        data: PyTorch Geometric data object containing labels and masks.\n",
    "\n",
    "    Returns:\n",
    "        Micro-F1 and Macro-F1 scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Map node IDs to indices in embeddings\n",
    "    node_to_idx_a = {node_id.item(): idx for idx, node_id in enumerate(aa_node_ids_a)}\n",
    "    node_to_idx_b = {node_id.item(): idx for idx, node_id in enumerate(aa_node_ids_b)}\n",
    "\n",
    "    # Reorder embeddings to match the label ordering\n",
    "    ordered_embeddings_a = np.zeros((len(data.y), aa_embeddings_a.shape[1]))\n",
    "    ordered_embeddings_b = np.zeros((len(data.y), aa_embeddings_b.shape[1]))\n",
    "\n",
    "    for i in range(len(data.y)):\n",
    "        ordered_embeddings_a[i] = aa_embeddings_a[node_to_idx_a[i]]\n",
    "        ordered_embeddings_b[i] = aa_embeddings_b[node_to_idx_b[i]]\n",
    "\n",
    "    ordered_embeddings = np.zeros((len(data.y), aa_embeddings_a.shape[1]))\n",
    "    if agg_type == 'sum':\n",
    "        ordered_embeddings = ordered_embeddings_a+ordered_embeddings_b\n",
    "    elif agg_type == 'concat':\n",
    "        if isinstance(ordered_embeddings_a, np.ndarray):\n",
    "            ordered_embeddings_a = torch.tensor(ordered_embeddings_a, dtype=torch.float32)\n",
    "\n",
    "        if isinstance(ordered_embeddings_b, np.ndarray):\n",
    "            ordered_embeddings_b = torch.tensor(ordered_embeddings_b, dtype=torch.float32)\n",
    "        ordered_embeddings = torch.cat([ordered_embeddings_a, ordered_embeddings_b],dim=1)\n",
    "    elif agg_type == 'avg':\n",
    "        ordered_embeddings = (ordered_embeddings_a+ordered_embeddings_b)/2\n",
    "    else: \n",
    "        raise ValueError('Invalid agg_type, choose from sum/concat/avg')\n",
    "    # Convert masks to NumPy arrays\n",
    "    train_mask = data.train_mask.numpy()\n",
    "    test_mask = data.test_mask.numpy()\n",
    "\n",
    "    # Prepare train and test data\n",
    "    X_train, X_test = ordered_embeddings[train_mask], ordered_embeddings[test_mask]\n",
    "    y_train, y_test = data.y[train_mask].numpy(), data.y[test_mask].numpy()\n",
    "\n",
    "    # Train Logistic Regression classifier\n",
    "    clf = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Compute Micro-F1 and Macro-F1 scores\n",
    "    micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    return {\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On  Structural Graph:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:03<00:00,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-F1: 0.6808 ± 0.0069\n",
      "Macro-F1: 0.6777 ± 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('On  Structural Graph:')\n",
    "struc_edges = data.edge_index\n",
    "a = []\n",
    "b = []\n",
    "for i in tqdm(range(16)):\n",
    "    aa_node_ids, aa_embeddings = get_embeddings(struc_edges, \"struc_edgelist_late_fusion.txt\", k=256, partition=1)\n",
    "    result = evaluate_node_classification(aa_node_ids, aa_embeddings, data)\n",
    "    a.append(result['micro_f1'])\n",
    "    b.append(result['macro_f1'])\n",
    "\n",
    "print(f\"Micro-F1: {np.mean(a):.4f} ± {np.std(a):.4f}\")\n",
    "print(f\"Macro-F1: {np.mean(b):.4f} ± {np.std(b):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community as community_louvain\n",
    "\n",
    "def LouvainNe_Embedding(graph):\n",
    "    dendrogram = community_louvain.generate_dendrogram(graph)\n",
    "\n",
    "    # Define parameters\n",
    "    d = 128  # Embedding dimension\n",
    "    a = 0.5  # Decay factor for hierarchical levels\n",
    "\n",
    "    # Store unique embeddings for each cluster at each level\n",
    "    level_embeddings = {}\n",
    "\n",
    "    # Assign a unique d-dimensional vector to each cluster at each level\n",
    "    for level in range(len(dendrogram)):\n",
    "        unique_clusters = set(dendrogram[level].values())  # Unique clusters at this level\n",
    "        level_embeddings[level] = {c: np.random.normal(0, 1, d) for c in unique_clusters}\n",
    "\n",
    "    # Compute final node embeddings\n",
    "    node_embeddings = {}\n",
    "\n",
    "    for node in graph.nodes():\n",
    "        final_embedding = np.zeros(d)  # Initialize embedding\n",
    "        current_cluster = None  # Track cluster as we go down\n",
    "\n",
    "        # Traverse from top level to bottom\n",
    "        for level in reversed(range(len(dendrogram))):  # Start from topmost level\n",
    "            if node in dendrogram[level]:\n",
    "                current_cluster = dendrogram[level][node]  # Assign cluster if found\n",
    "\n",
    "            if current_cluster is not None:\n",
    "                final_embedding += (a ** (level + 1)) * level_embeddings[level][current_cluster]\n",
    "\n",
    "        # Add final standard normal noise\n",
    "        final_embedding += np.random.normal(0, 1, d)\n",
    "\n",
    "        node_embeddings[node] = final_embedding\n",
    "\n",
    "    # Convert to numpy array (for ML models)\n",
    "    node_embedding_matrix = np.array([node_embeddings[node] for node in graph.nodes()])\n",
    "\n",
    "    return node_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On  Structural Graph:\n",
      "(2708, 128)\n"
     ]
    }
   ],
   "source": [
    "print('On  Structural Graph:')\n",
    "struc_graph = to_networkx(data, to_undirected=True)\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "aa_embeddings = LouvainNe_Embedding(struc_graph)\n",
    "print(aa_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('On  Structural Graph:')\n",
    "struc_edges = data.edge_index\n",
    "struc_graph = nx.Graph()\n",
    "edges = list(zip(struc_edges[0], struc_edges[1]))\n",
    "struc_graph.add_edges_from(edges)\n",
    "a = []\n",
    "b = []\n",
    "for i in tqdm(range(16)):\n",
    "    aa_node_ids, aa_embeddings = get_embeddings(struc_edges, \"struc_edgelist_late_fusion.txt\", k=256, partition=1)\n",
    "    result = evaluate_node_classification(aa_node_ids, aa_embeddings, data)\n",
    "    a.append(result['micro_f1'])\n",
    "    b.append(result['macro_f1'])\n",
    "\n",
    "print(f\"Micro-F1: {np.mean(a):.4f} ± {np.std(a):.4f}\")\n",
    "print(f\"Macro-F1: {np.mean(b):.4f} ± {np.std(b):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_graph = nx.Graph()\n",
    "edges = list(zip(attr_edges[0], attr_edges[1]))\n",
    "attr_graph.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Attribute Graph:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:32<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-F1: 0.2760 ± 0.0000\n",
      "Macro-F1: 0.2192 ± 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('On Attribute Graph:')\n",
    "a = []\n",
    "b = []\n",
    "for i in tqdm(range(16)):\n",
    "    aa_node_ids, aa_embeddings = get_embeddings(attr_edges, \"attr_edgelist_late_fusion.txt\", k=256, partition=1)\n",
    "    result = evaluate_node_classification(aa_node_ids, aa_embeddings, data)\n",
    "    a.append(result['micro_f1'])\n",
    "    b.append(result['macro_f1'])\n",
    "\n",
    "print(f\"Micro-F1: {np.mean(a):.4f} ± {np.std(a):.4f}\")\n",
    "print(f\"Macro-F1: {np.mean(b):.4f} ± {np.std(b):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Late Fusion (Sum):')\n",
    "a = []\n",
    "b = []\n",
    "for i in tqdm(range(128)):\n",
    "    aa_node_ids_struc, aa_embeddings_struc = get_embeddings(struc_edges, \"struc_edgelist_late_fusion.txt\", k=512, partition=1)\n",
    "    aa_node_ids_attr, aa_embeddings_attr = get_embeddings(mapped_attr_edges, \"attr_edgelist_late_fusion.txt\", k=512, partition=1)\n",
    "    # reverse mapping the indices from virtual index to actual index\n",
    "    org_aa_node_ids_attr = torch.tensor([rev_map_attr_edges[idx.item()] for idx in aa_node_ids_attr])\n",
    "    aa_node_ids_attr, aa_embeddings_attr = append(org_aa_node_ids_attr, aa_embeddings_attr, data)\n",
    "    agg_type='concat' # sum/concat/avg\n",
    "    result = evaluate_node_classification_latefusion(aa_node_ids_struc, aa_embeddings_struc, aa_node_ids_attr, aa_embeddings_attr, data, agg_type)\n",
    "    a.append(result['micro_f1'])\n",
    "    b.append(result['macro_f1'])\n",
    "\n",
    "print(f\"Micro-F1: {np.mean(a):.4f} ± {np.std(a):.4f}\")\n",
    "print(f\"Macro-F1: {np.mean(b):.4f} ± {np.std(b):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_node_ids_struc, aa_embeddings_struc = get_embeddings(struc_edges, \"struc_edgelist_late_fusion.txt\", k=256, partition=1)\n",
    "aa_node_ids_attr, aa_embeddings_attr = get_embeddings(mapped_attr_edges, \"attr_edgelist_late_fusion.txt\", k=256, partition=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708]) torch.Size([2708, 256])\n",
      "torch.Size([1434]) torch.Size([1434, 256])\n"
     ]
    }
   ],
   "source": [
    "print(aa_node_ids_struc.shape, aa_embeddings_struc.shape)\n",
    "print(aa_node_ids_attr.shape, aa_embeddings_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse mapping the indices from virtual index to actual index\n",
    "org_aa_node_ids_attr = torch.tensor([rev_map_attr_edges[idx.item()] for idx in aa_node_ids_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,  189,   29, 1296,  115,  769,  870, 1181, 1276,    9],\n",
      "       dtype=torch.int32)\n",
      "tensor([   1,  332,   48, 2406,  197, 1446, 1615, 2214, 2381,   17])\n"
     ]
    }
   ],
   "source": [
    "print(aa_node_ids_attr[:10])\n",
    "print(org_aa_node_ids_attr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_node_ids_attr, aa_embeddings_attr = append(org_aa_node_ids_attr, aa_embeddings_attr, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_type='concat' # sum/concat/avg\n",
    "result = evaluate_node_classification_latefusion(aa_node_ids_struc, aa_embeddings_struc, aa_node_ids_attr, aa_embeddings_attr, data, agg_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'micro_f1': 0.636, 'macro_f1': 0.6343200813275683}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StableVITON",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
